{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import gym\n",
    "from collections import namedtuple\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import random\n",
    "import math\n",
    "from itertools import count\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.distributions import Categorical\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class policyNet(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(policyNet,self).__init__()\n",
    "        self.L1 = nn.Linear(4,30)\n",
    "        self.out = nn.Linear(30,2)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x = F.relu(self.L1(x))\n",
    "        x = self.out(x)\n",
    "        x = F.softmax(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fung/.local/lib/python2.7/site-packages/ipykernel_launcher.py:11: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v0').unwrapped\n",
    "policy_net = policyNet()\n",
    "optimizer = torch.optim.RMSprop(policy_net.parameters() , lr = 0.01)\n",
    "batch_size = 16\n",
    "SAR = namedtuple('sar',['state','action','reward'])\n",
    "steps = 0\n",
    "duration = []\n",
    "SAR_list = []\n",
    "try:\n",
    "    for e in range(200):\n",
    "        state = env.reset()\n",
    "        state = torch.Tensor(state)\n",
    "        for t in count():\n",
    "            env.render()\n",
    "            probs = policy_net(state)\n",
    "            m = Categorical(probs)\n",
    "            action = m.sample()\n",
    "            next_state , reward , done , _ = env.step(action.item())\n",
    "\n",
    "            if done:\n",
    "                reward = 0\n",
    "            sar = SAR(state , action , reward)\n",
    "            SAR_list.append(sar)\n",
    "            state = next_state\n",
    "            state = torch.Tensor(state)\n",
    "            \n",
    "            steps += 1\n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        duration.append(t)\n",
    "        \n",
    "        if e>0 and e%batch_size == 0:\n",
    "            \n",
    "            rewards = np.zeros([steps])\n",
    "            gamma = 0.99\n",
    "            cur_reward = 0\n",
    "            \n",
    "            for i in reversed(range(steps)):\n",
    "                sar = SAR_list[i]\n",
    "                r = sar.reward\n",
    "                if r == 0:\n",
    "                    cur_reward = 0\n",
    "                else:\n",
    "                    cur_reward = gamma*cur_reward + r\n",
    "                    rewards[i] = cur_reward\n",
    "                    \n",
    "            reward_mean = np.mean(rewards)\n",
    "            reward_std = np.std(rewards)\n",
    "            rewards = (rewards-reward_mean) / reward_std\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss = 0\n",
    "            for i in range(steps):\n",
    "                # TODO:\n",
    "                # Take out state, action, reward from SAR_list\n",
    "                # Compute loss\n",
    "                sar = SAR_list[i]\n",
    "                cur_state = sar.state\n",
    "                cur_action = sar.action\n",
    "                cur_reward = rewards[i]\n",
    "                \n",
    "                probs = policy_net(cur_state)           # <= hint: feed something into policy network\n",
    "                m = Categorical(probs)\n",
    "                loss += -m.log_prob( cur_action ) * cur_reward\n",
    "                # END TODO\n",
    "            loss /= batch_size\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            steps = 0\n",
    "            SAR_list = []\n",
    "            \n",
    "finally:\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fung/.local/lib/python2.7/site-packages/ipykernel_launcher.py:11: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "env = gym.make('CartPole-v0').unwrapped\n",
    "state = env.reset()\n",
    "state = torch.Tensor(state)\n",
    "frames = []\n",
    "try:\n",
    "    for t in count():\n",
    "        env.render()\n",
    "        frames.append(Image.fromarray(env.render(mode='rgb_array')))\n",
    "        probs = policy_net(state)\n",
    "        m = Categorical(probs)\n",
    "        action = m.sample()\n",
    "        state , reward , done , _ = env.step(action.item())\n",
    "        state = torch.Tensor(state)\n",
    "        if done:\n",
    "            break\n",
    "finally:\n",
    "    env.close()\n",
    "    with open('policy_gradient.gif','wb') as f:\n",
    "        im = Image.new('RGB', frames[0].size)\n",
    "        im.save(f, save_all=True, append_images=frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
